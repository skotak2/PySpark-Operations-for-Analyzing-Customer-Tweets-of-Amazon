# -*- coding: utf-8 -*-
"""Copy of Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DOys9lRlVIOokIIBhTQELL9bnMuobSId
"""

from google.colab import drive
drive.mount('/content/drive')

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

!wget -q http://apache.mirrors.pair.com/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz

!ls

!tar -xvf spark-2.4.5-bin-hadoop2.7.tgz

!ls

!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.5-bin-hadoop2.7"

import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
import pandas as pd

sc = spark.sparkContext
myrdd = sc.textFile("/content/drive/My Drive/Amazon_Responded_Oct05.csv", )
m1 = myrdd.map(lambda line: line.split(','))
m2 = m1.filter(lambda line: len(line)==25)\
    .map(lambda line: (line[0],line[1],line[11],line[17],line[22],line[16]))\
    .filter(lambda line: line[2]=='False')\
    .filter(lambda line: len(line[1].split(' ')) == 6 )\
    .map(lambda line: ((line[1].split(' ')[1]+line[1].split(' ')[2]),(line[0],line[2],line[3],line[4],line[5])))\
    .countByKey().items()
list = m1.first()
a = []
a.append(list.index('id_str'))
a.append(list.index('tweet_created_at'))
a.append(list.index('user_verified'))
a.append(list.index('favorite_count'))
a.append(list.index('retweet_count'))
a.append(list.index('text_'))
a
dt = pd.DataFrame.from_dict(m2)

dt.iloc[dt[1].idxmax()]
m3 = m1.filter(lambda line: len(line)==25)\
    .map(lambda line: (line[0],line[1],line[11],line[17],line[22],line[16]))\
    .filter(lambda line: line[2]=='False')\
    .filter(lambda line: len(line[1].split(' ')) == 6 )\
    .map(lambda line: ((line[1].split(' ')[1]+line[1].split(' ')[2]),(line[0],line[2],line[3],line[4],line[5])))
    m4 = m3.filter(lambda line: line[0] == 'Jan03')\
     .map(lambda line: (int(line[1][2])+int(line[1][3]),line[1][0]))\
     .collect()

from pyspark import SparkContext
import pyspark.sql.functions
from pyspark.sql import SQLContext
import pandas as pd
sc = spark.sparkContext

scsql = SQLContext(sc)

df = scsql.read.csv("/content/drive/My Drive/Amazon_Responded_Oct05.csv", header=True)

df.columns

dt = df.select('id_str','tweet_created_at','user_verified','favorite_count','retweet_count','text')
dt.show(5)

dt1 = dt.filter(dt.user_verified == 'True')
dt1.show(5)

split_date = pyspark.sql.functions.split(dt1['tweet_created_at'],' ') 
sf = pyspark.sql.functions 
df= dt1.withColumn('Month', split_date.getItem(1))
df=df.withColumn('Day',split_date.getItem(2))
df=df.withColumn('Date',sf.concat('Month','Day'))
df =df.select('id_str','Date','user_verified','favorite_count','retweet_count','text')

df1 = df.groupBy("Date")\
     .agg( 
          sf.count(sf.lit(1)).alias("Num Of Records")
         )\
     .orderBy("Num Of Records", ascending=False)\
     .show(20, False)

df1 = df.groupBy("Date")\
     .agg( 
          sf.count(sf.lit(1)).alias("Num Of Records")
         )\
     .orderBy("Num Of Records", ascending=False)\
     .limit(100)

df_pd = df1.toPandas()

dt.filter(dt.user_verified == 'True')
df2 = df.select('id_str','Date','user_verified','favorite_count','retweet_count','text')\
      .filter(df.user_verified == 'True')\
      .filter(df.Date == 'Jan03')\
      .withColumn('Sum',df['favorite_count']+df['retweet_count'])\
      .orderBy("Sum", ascending=False)\
      .limit(99)

df3 = df2.select('text').toPandas()

df4 = df3['text'].tolist()

import nltk
import pandas as pd
nltk.download('stopwords')
nltk.download('punkt')
from nltk.tokenize import WordPunctTokenizer
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import PorterStemmer 
from nltk.corpus import stopwords
import string

test_list = df4
df6 = [i for i in test_list if i]    
df6 = [word_tokenize(i) for i in df6]   
df6 = [w for i in df6 for w in i if w.isalpha()]
df6
wordfreq = {}
for j in range(len(df6)):
    tokens = df6[j]   
    if tokens not in wordfreq.keys():
      wordfreq[tokens] = 1        
    else:
      wordfreq[tokens] += 1   
wordfreq

df_find = scsql.read.csv("/content/drive/My Drive/find_text.csv", header=True)
df_dt = df_find.toPandas()

df_find = df_find.selectExpr("id_str as id")
df_find.show()

dt.show()

inner_join = df_find.join(dt, df_find.id == dt.id_str)
final = inner_join.select('id_str','text').toPandas()

final.to_csv(r"/content/drive/My Drive/find_text_answrs.csv")

df_final = inner_join.select('text').toPandas()
df_final

df_dt